{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning Model Evaluation Lab\n",
    "\n",
    "Complete the exercises below to solidify your knowledge and understanding of supervised learning model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_boston()\n",
    "\n",
    "X = pd.DataFrame(data[\"data\"], columns=data[\"feature_names\"])\n",
    "y = pd.DataFrame(data[\"target\"], columns=['MEDV'])\n",
    "\n",
    "data = pd.concat([X, y], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Split this data set into training (80%) and testing (20%) sets.\n",
    "\n",
    "The `MEDV` field represents the median value of owner-occupied homes (in $1000's) and is the target variable that we will want to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train a `LinearRegression` model on this data set and generate predictions on both the training and the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg = LinearRegression()\n",
    "linreg.fit(x_train, y_train)\n",
    "y_pred_train = linreg.predict(x_train)\n",
    "y_pred_test = linreg.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Calculate and print R-squared for both the training and the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7334492147453092 0.738339392059052\n"
     ]
    }
   ],
   "source": [
    "test_r2score = metrics.r2_score(y_test, y_pred_test)  \n",
    "train_r2score = metrics.r2_score(y_train, y_pred_train)  \n",
    "print(test_r2score, train_r2score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Calculate and print mean squared error for both the training and the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.869292183770682 22.477090408387628\n"
     ]
    }
   ],
   "source": [
    "test_msescore = metrics.mean_squared_error(y_test, y_pred_test)  \n",
    "train_msescore = metrics.mean_squared_error(y_train, y_pred_train)  \n",
    "print(test_msescore, train_msescore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Calculate and print mean absolute error for both the training and the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.21327049584237 3.3500095196484505\n"
     ]
    }
   ],
   "source": [
    "test_maescore = metrics.mean_absolute_error(y_test, y_pred_test)  \n",
    "train_maescore = metrics.mean_absolute_error(y_train, y_pred_train)  \n",
    "print(test_maescore, train_maescore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "data = load_iris()\n",
    "\n",
    "X = pd.DataFrame(data[\"data\"], columns=data[\"feature_names\"])\n",
    "y = pd.DataFrame(data[\"target\"], columns=[\"class\"])\n",
    "\n",
    "data = pd.concat([X, y], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Split this data set into training (80%) and testing (20%) sets.\n",
    "\n",
    "The `class` field represents the type of flower and is the target variable that we will want to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.drop('class', axis = 1)\n",
    "y = data['class']\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train a `LogisticRegression` model on this data set and generate predictions on both the training and the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(solver = 'liblinear', max_iter = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.fit(x_train, y_train)\n",
    "y_test_pred = logreg.predict(x_test)\n",
    "y_train_pred = logreg.predict(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Calculate and print the accuracy score for both the training and the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9 0.9583333333333334\n"
     ]
    }
   ],
   "source": [
    "accuracy_test = metrics.accuracy_score(y_test, y_test_pred)\n",
    "accuracy_train = metrics.accuracy_score(y_train, y_train_pred)\n",
    "print(accuracy_test, accuracy_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Calculate and print the balanced accuracy score for both the training and the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9090909090909092 0.9572649572649573\n"
     ]
    }
   ],
   "source": [
    "balaccuracy_test = metrics.balanced_accuracy_score(y_test, y_test_pred)\n",
    "balaccuracy_train = metrics.balanced_accuracy_score(y_train, y_train_pred)\n",
    "print(balaccuracy_test, balaccuracy_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Calculate and print the precision score for both the training and the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9285714285714285 0.958994708994709\n",
      "0.9 0.9583333333333334\n",
      "0.9214285714285714 0.9600198412698411\n"
     ]
    }
   ],
   "source": [
    "prec_test = metrics.precision_score(y_test, y_test_pred, average = 'macro')\n",
    "prec_train = metrics.precision_score(y_train, y_train_pred, average = 'macro')\n",
    "print(prec_test, prec_train)\n",
    "\n",
    "prec_test = metrics.precision_score(y_test, y_test_pred, average = 'micro')\n",
    "prec_train = metrics.precision_score(y_train, y_train_pred, average = 'micro')\n",
    "print(prec_test, prec_train)\n",
    "\n",
    "prec_test = metrics.precision_score(y_test, y_test_pred, average = 'weighted')\n",
    "prec_train = metrics.precision_score(y_train, y_train_pred, average = 'weighted')\n",
    "print(prec_test, prec_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Calculate and print the recall score for both the training and the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9090909090909092 0.9572649572649573\n",
      "0.9 0.9583333333333334\n",
      "0.9 0.9583333333333334\n"
     ]
    }
   ],
   "source": [
    "recall_test = metrics.recall_score(y_test, y_test_pred, average = 'macro')\n",
    "recall_train = metrics.recall_score(y_train, y_train_pred, average = 'macro')\n",
    "print(recall_test, recall_train)\n",
    "recall_test = metrics.recall_score(y_test, y_test_pred, average = 'micro')\n",
    "recall_train = metrics.recall_score(y_train, y_train_pred, average = 'micro')\n",
    "print(recall_test, recall_train)\n",
    "recall_test = metrics.recall_score(y_test, y_test_pred, average = 'weighted')\n",
    "recall_train = metrics.recall_score(y_train, y_train_pred, average = 'weighted')\n",
    "print(recall_test, recall_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Calculate and print the F1 score for both the training and the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9073684210526315 0.9572016460905349\n",
      "0.9 0.9583333333333334\n",
      "0.8981052631578947 0.9582716049382716\n"
     ]
    }
   ],
   "source": [
    "f1_test = metrics.f1_score(y_test, y_test_pred, average = 'macro')\n",
    "f1_train = metrics.f1_score(y_train, y_train_pred, average = 'macro' )\n",
    "print(f1_test, f1_train)\n",
    "f1_test = metrics.f1_score(y_test, y_test_pred, average = 'micro')\n",
    "f1_train = metrics.f1_score(y_train, y_train_pred, average = 'micro' )\n",
    "print(f1_test, f1_train)\n",
    "f1_test = metrics.f1_score(y_test, y_test_pred, average = 'weighted')\n",
    "f1_train = metrics.f1_score(y_train, y_train_pred, average = 'weighted' )\n",
    "print(f1_test, f1_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Generate confusion matrices for both the training and the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Setosa</th>\n",
       "      <th>Versicolour</th>\n",
       "      <th>Virginica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Setosa</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Versicolour</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Virginica</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Setosa  Versicolour  Virginica\n",
       "Setosa            8            0          0\n",
       "Versicolour       0            8          3\n",
       "Virginica         0            0         11"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(metrics.confusion_matrix(y_test, y_test_pred), columns =['Setosa','Versicolour','Virginica'], index =['Setosa','Versicolour','Virginica'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Setosa</th>\n",
       "      <th>Versicolour</th>\n",
       "      <th>Virginica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Setosa</th>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Versicolour</th>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Virginica</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Setosa  Versicolour  Virginica\n",
       "Setosa           42            0          0\n",
       "Versicolour       0           35          4\n",
       "Virginica         0            1         38"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(metrics.confusion_matrix(y_train, y_train_pred), columns =['Setosa','Versicolour','Virginica'], index =['Setosa','Versicolour','Virginica'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: For each of the data sets in this lab, try training with some of the other models you have learned about, recalculate the evaluation metrics, and compare to determine which models perform best on each data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "kn5 = KNeighborsClassifier(n_neighbors = 3)\n",
    "kn5.fit(x_train, y_train)\n",
    "kn5_y_predtest = kn5.predict(x_test)\n",
    "kn5_y_predtrain = kn5.predict(x_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      " [[ 8  0  0]\n",
      " [ 0  9  2]\n",
      " [ 0  0 11]]\n",
      "Accuracy Score 0.9333333333333333\n",
      "Balanced accuracy score 0.9393939393939394\n"
     ]
    }
   ],
   "source": [
    "print('Confusion matrix\\n',metrics.confusion_matrix(y_test, kn5_y_predtest))\n",
    "print('Accuracy Score', metrics.accuracy_score(y_test, kn5_y_predtest))\n",
    "print('Balanced accuracy score',metrics.balanced_accuracy_score(y_test, kn5_y_predtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      " [[42  0  0]\n",
      " [ 0 38  1]\n",
      " [ 0  2 37]]\n",
      "Accuracy Score 0.975\n",
      "Balanced accuracy score 0.9743589743589745\n"
     ]
    }
   ],
   "source": [
    "print('Confusion matrix\\n',metrics.confusion_matrix(y_train, kn5_y_predtrain))\n",
    "print('Accuracy Score', metrics.accuracy_score(y_train, kn5_y_predtrain))\n",
    "print('Balanced accuracy score',metrics.balanced_accuracy_score(y_train, kn5_y_predtrain))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
